_wandb:
    value:
        cli_version: 0.18.6
        m:
            - "1": trainer/global_step
              "6":
                - 3
              "7": []
            - "1": train_loss
              "5": 1
              "6":
                - 1
                - 3
              "7": []
            - "1": epoch
              "5": 1
              "6":
                - 1
                - 3
              "7": []
            - "1": val_loss
              "5": 1
              "6":
                - 1
                - 3
              "7": []
        python_version: 3.10.12
        t:
            "1":
                - 1
                - 9
                - 11
                - 49
                - 51
                - 55
                - 103
            "2":
                - 1
                - 9
                - 11
                - 49
                - 51
                - 55
                - 103
            "3":
                - 5
                - 7
                - 13
                - 14
                - 16
                - 23
                - 55
                - 62
                - 66
            "4": 3.10.12
            "5": 0.18.6
            "6": 4.44.2
            "8":
                - 5
            "12": 0.18.6
            "13": linux-x86_64
ExpName:
    value: pt-t5-small-naive-L5
LightningDataModuleName:
    value: GEMSGD_DataModule
LightningDataModuleParas:
    value:
        batch_size: 8
        encode_args:
            padding: max_length
            truncation: true
        force_process: false
        linearizer_class: SGD_PaperNaiveLinearizer
        save_cache: true
        schema_paths:
            - data/schemas/schema-train.json
            - data/schemas/schema-test.json
            - data/schemas/schema-dev.json
        template_dir: data/utterance_templates
        tokenizer_name: T5TokenizerFast
LightningModuleName:
    value: PrefixT5GenerationModel
LightningModuleParas:
    value:
        hidden_dims:
            - 512
            - 384
        model_class: PT_T5Model
        model_path: new
        optimizer: AdamW
        optimizer_params:
            lr: 0.0001
        prefix_length: 5
LoadingParas:
    value:
        checkpoint_path: bbyrne-nlg/train-1-fix_pos/checkpoints/epoch=4-step=102491.ckpt
        decode_path: logs/pt-t5-small-naive-L5/test-pt-t5-naive-L5-1
        generate_params:
            bos_token_id: 0
            early_stopping: true
            length_penalty: 0.6
            max_new_tokens: 1000
            num_beams: 4
        save_decode: true
ModelCheckpointParas:
    value:
        monitor: val_loss
ModelName:
    value: pt-t5-small-SGD
Note:
    value: prefix t5-small for SGD, using naive SR
TestLoggerName:
    value: CSVLogger
TestLoggerParas:
    value:
        name: pt-t5-small-naive-L5
        version: train-1-fix_pos
TokenizerInfo:
    value:
        tokenizer_alias: T5TokenizerFast
        tokenizer_class: T5TokenizerFast
        tokenizer_name: t5-small
TrainLoggerName:
    value: WandbLogger
TrainLoggerParas:
    value:
        name: pt-t5-small-naive-L5
        project: bbyrne-nlg
        version: train-1-fix_pos
TrainerParas:
    value:
        accelerator: gpu
        default_root_dir: models/pt-t5-small-SGD
        devices: 1
        max_epochs: 5
        val_check_interval: 10000
VerName:
    value: train-1-fix_pos
WandbProject:
    value: bbyrne-nlg
activation_fn_name:
    value: Tanh
decode_path:
    value: default_test/
generate_params:
    value: null
hidden_dims:
    value:
        - 512
        - 384
model_class:
    value: PT_T5Model
model_path:
    value: new
optimizer:
    value: AdamW
optimizer_params:
    value:
        lr: 0.0001
prefix_dropout:
    value: 0
prefix_length:
    value: 5
prefix_model_name:
    value: MLPPrefixModel
save_decode:
    value: false
t5_model_name:
    value: t5-small
tokenizer:
    value: null
